# ðŸš€ Narrator Context Caching Strategy

> **Performance optimization plan to reduce `/narrator/context` latency from 10+ seconds to <2 seconds**

## ðŸŽ¯ Executive Summary

The narrator context endpoint currently takes 10+ seconds due to redundant external API calls and lack of caching. This document outlines a comprehensive caching strategy that will:

- **Eliminate triple redundancy** in Beeminder API calls
- **Cache expensive operations** with intelligent invalidation  
- **Maintain data freshness** with appropriate TTLs
- **Support stateless deployment** (Spin app ready)

**Target Performance:** Reduce latency from 10+ seconds to <2 seconds for cached requests.

---

## ðŸ” Current Performance Analysis

### Data Sources in `/narrator/context`

| Component | Type | Current Behavior | Est. Latency | Cache Status |
|-----------|------|------------------|--------------|--------------|
| **Beeminder Goals** | Network API | Called 3x per request | 3-6 seconds | âŒ None |
| **Obsidian Todos** | Network MCP | Vault scan + file reads | 2-4 seconds | âŒ None |
| **Local Goals** | SQLite | Direct query | <50ms | âœ… N/A |
| **Budget Status** | SQLite | Direct query | <50ms | âœ… N/A |
| **Daily Activity** | Network API | Single call | 200-500ms | âœ… 1h TTL |

### Critical Issues Identified

1. **ðŸ”¥ TRIPLE REDUNDANCY**: `beeminder_client.get_all_goals()` called 3 times:
   - Once by `get_all_goals()`
   - Again by `get_emergencies()` 
   - Again by `get_runway_summary()`

2. **ðŸ”¥ OBSIDIAN VAULT SCANNING**: Full vault search for every todo request:
   - Search for `"- [ ]"` patterns (entire vault)
   - Search for `"- [x]"` patterns (entire vault)
   - Read content of every matching file

3. **ðŸ”¥ NO INTERMEDIATE CACHING**: No caching between expensive operations

---

## ðŸ› ï¸ Caching Strategy by Component

### 1. Beeminder Goals Cache

**Priority:** ðŸ”¥ **CRITICAL** (Highest impact)

**Current Problem:**
```python
# In get_narrator_context() - CALLED 3 TIMES!
beeminder_status = await beeminder_client.get_all_goals()      # Call 1
emergencies = await beeminder_client.get_emergencies()         # Call 2 â†’ calls get_all_goals() again!
goal_runway = await beeminder_client.get_runway_summary()      # Call 3 â†’ calls get_all_goals() again!
```

**Solution: Single Call + Derived Data**
```python
# Cache the base data once
beeminder_goals = await get_cached_beeminder_goals()

# Derive other views from cached data
beeminder_status = beeminder_goals  
emergencies = derive_emergencies(beeminder_goals)
goal_runway = derive_runway_summary(beeminder_goals, limit=4)
```

**Cache Configuration:**
- **TTL:** 30 minutes (Beeminder updates infrequently)
- **Key:** `beeminder_goals_{username}`
- **Invalidation:** Time-based + manual endpoint for emergencies
- **Fallback:** Return stale data on API failure, flag as stale

**Implementation Location:** `beeminder_client.py`

### 2. Obsidian Todos Cache

**Priority:** ðŸ”¥ **HIGH** (Second highest impact)

**Current Problem:**
```python
# Scans entire vault TWICE per request
checkbox_matches = await self.search_vault("- [ ]")        # Full vault scan
completed_matches = await self.search_vault("- [x]")       # Full vault scan  
# Then reads EVERY matching file individually
```

**Solution: Cached Todo Index**
- Cache the complete todo index, not individual searches
- Include file modification times for smart invalidation
- Pre-parse and structure todo data

**Cache Configuration:**
- **TTL:** 15 minutes (todos change frequently during work sessions)
- **Key:** `obsidian_todos_{vault_hash}`
- **Invalidation:** File modification time tracking (when possible)
- **Fallback:** Return stale data if Obsidian MCP unavailable

**Implementation Location:** `obsidian_client.py`

### 3. Daily Activity Cache

**Priority:** âœ… **DONE** (Already implemented)

**Current Status:** âœ… Working well with 1-hour cache

**Notes:** This is our success model - extend this pattern to other components.

### 4. Narrator Context Assembly Cache

**Priority:** ðŸŸ¡ **MEDIUM** (Performance optimization)

**Strategy:** Cache the final assembled context response

**Cache Configuration:**
- **TTL:** 5 minutes (short, for responsiveness)
- **Key:** `narrator_context_{hash_of_inputs}`
- **Invalidation:** Automatic when any underlying cache expires
- **Purpose:** Serve repeated requests instantly

---

## ðŸ—ï¸ Implementation Architecture

### Phase 1: In-Memory Caching (Start Here)

**Target:** Quick wins with minimal infrastructure changes

```python
# Shared cache structure in mcp_server.py
app_cache = {
    "beeminder_goals": {
        "data": None,
        "expires": None,
        "last_check": None
    },
    "obsidian_todos": {
        # Similar structure
    }
}
```

**Benefits:**
- âœ… Immediate performance gains
- âœ… Simple to implement
- âœ… Works with current architecture
- âœ… Testable and debuggable

**Limitations:**
- âŒ Lost on process restart
- âŒ No sharing between instances

### Phase 2: Persistent Caching (Future)

**Target:** Spin app deployment readiness

**Options Considered:**
1. **SQLite Cache Table** (Recommended)
   - âœ… Already using SQLite for other data
   - âœ… ACID transactions
   - âœ… Simple expiration logic
   - âœ… Survives restarts

2. **Redis** (If needed later)
   - âœ… Distributed caching
   - âœ… Advanced features
   - âŒ Additional infrastructure dependency

3. **File-based Cache**
   - âœ… Simple
   - âŒ Concurrency issues
   - âŒ Manual cleanup needed

---

## ðŸ“‹ Implementation Plan

### Phase 1: Emergency Fixes (1-2 hours)

**Goal:** Eliminate triple redundancy immediately

1. **Refactor Beeminder Client**
   - [ ] Create `get_cached_beeminder_goals()` method
   - [ ] Modify `get_emergencies()` to accept pre-fetched goals
   - [ ] Modify `get_runway_summary()` to accept pre-fetched goals
   - [ ] Update `/narrator/context` to call once + derive

**Expected Improvement:** 10 seconds â†’ 6 seconds (40% reduction)

### Phase 2: Beeminder Goals Caching (2-3 hours)

2. **Add In-Memory Beeminder Cache**
   - [ ] Implement cache storage structure
   - [ ] Add TTL checking logic  
   - [ ] Add cache invalidation endpoint
   - [ ] Add fallback logic for API failures

**Expected Improvement:** 6 seconds â†’ 3 seconds (50% reduction)

### Phase 3: Obsidian Todos Caching (3-4 hours)

3. **Add In-Memory Obsidian Cache**
   - [ ] Implement cached todo index
   - [ ] Add smart invalidation (if file modification times available)
   - [ ] Add fallback logic for MCP server failures

**Expected Improvement:** 3 seconds â†’ 1.5 seconds (50% reduction)

### Phase 4: Optimization & Monitoring (2-3 hours)

4. **Final Optimizations**
   - [ ] Add narrator context assembly cache (5 min TTL)
   - [ ] Add cache hit/miss metrics to `/health` endpoint
   - [ ] Add cache management endpoints (`/cache/status`, `/cache/clear`)
   - [ ] Performance testing and tuning

**Expected Final Performance:** <2 seconds consistently

---

## ðŸŽ¯ Cache TTL Recommendations

| Component | TTL | Rationale |
|-----------|-----|-----------|
| **Beeminder Goals** | 30 min | Goals update infrequently, safebuf changes slowly |
| **Obsidian Todos** | 15 min | Active during work sessions, needs freshness |
| **Daily Activity** | 1 hour | âœ… Already working well |
| **Narrator Assembly** | 5 min | Final response cache for rapid repeated access |

### Emergency Override

- **Manual cache invalidation** endpoints for urgent updates
- **Beemergency detection** can force immediate cache refresh
- **Health check failures** can trigger cache invalidation

---

## ðŸ”’ Cache Safety Considerations

### 1. Graceful Degradation
```python
try:
    data = await get_cached_data()
except Exception:
    # Always fall back to live data for critical systems
    data = await get_live_data()
```

### 2. Stale Data Handling
- Mark stale data with timestamps
- Include staleness warnings in responses
- Set maximum stale age limits

### 3. Cache Consistency
- Version cache entries to handle schema changes
- Use content hashing for cache keys where appropriate
- Clear related caches when dependencies change

---

## ðŸ“Š Success Metrics

### Performance Targets
- **Latency:** <2 seconds for 95% of cached requests
- **Cache Hit Rate:** >80% during normal usage
- **API Call Reduction:** >70% reduction in external calls

### Monitoring Points
- Response time distribution
- Cache hit/miss ratios by component
- External API call frequency
- Error rates and fallback usage

---

## ðŸš€ Future Considerations

### Spin App Readiness
- Current Phase 1 (in-memory) â†’ Migration to SQLite cache tables
- Stateless design ensures no session affinity needed
- Cache warming strategies for cold starts

### Advanced Features (Post-Launch)
- **Smart prefetching** based on usage patterns
- **Incremental updates** for large datasets
- **Background refresh** to avoid cache misses
- **Cache sharding** for multi-user scenarios

---

## ðŸ’¡ Key Insights

1. **Biggest Win:** Eliminating triple redundancy in Beeminder calls provides immediate 40% improvement
2. **Caching Pattern:** The existing daily activity cache is the perfect model to extend
3. **Risk Mitigation:** Always fall back to live data - performance improvement should never compromise reliability
4. **Progressive Enhancement:** Each phase provides independent value, can be deployed incrementally

**Next Action:** Start with Phase 1 to get immediate 40% performance improvement with minimal risk.